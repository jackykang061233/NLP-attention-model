{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from termcolor import colored\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import trax\n",
    "from trax import layers as tl\n",
    "from trax.fastmath import numpy as fastnp\n",
    "from trax.supervised import training\n",
    "\n",
    "import w1_unittest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "/home/jovyan/.local/lib/python3.9/site-packages/jax/_src/xla_bridge.py:851: UserWarning: jax.host_count has been renamed to jax.process_count. This alias will eventually be removed; please update your code.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_stream_fn = trax.data.TFDS('opus/medical',\n",
    "                                 data_dir='./data/',\n",
    "                                 keys=('en', 'de'),\n",
    "                                 eval_holdout_size=0.01,\n",
    "                                 train=True\n",
    "                                 )\n",
    "eval_stream_fn = trax.data.TFDS('opus/medical',\n",
    "                                 data_dir='./data/',\n",
    "                                 keys=('en', 'de'),\n",
    "                                 eval_holdout_size=0.01,\n",
    "                                 train=False\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mtrain data (en, de) tuple:\u001b[0m (b'Tel: +421 2 57 103 777\\n', b'Tel: +421 2 57 103 777\\n')\n",
      "\n",
      "\u001b[31meval data (en, de) tuple:\u001b[0m (b'Subcutaneous use and intravenous use.\\n', b'Subkutane Anwendung und intraven\\xc3\\xb6se Anwendung.\\n')\n"
     ]
    }
   ],
   "source": [
    "train_stream = train_stream_fn()\n",
    "print(colored('train data (en, de) tuple:', 'red'), next(train_stream))\n",
    "print()\n",
    "\n",
    "eval_stream = eval_stream_fn()\n",
    "print(colored('eval data (en, de) tuple:', 'red'), next(eval_stream))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_FILE = 'ende_32k.subword'\n",
    "VOCAB_DIR = 'data/'\n",
    "\n",
    "tokenized_train_stream = trax.data.Tokenize(vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)(train_stream)\n",
    "tokenized_eval_stream = trax.data.Tokenize(vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)(eval_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append EOS at the end of each sentence\n",
    "EOS = 1\n",
    "\n",
    "def append_eos(stream):\n",
    "    for (inputs, targets) in stream:\n",
    "        inputs_with_eos = list(inputs) + [EOS]\n",
    "        targets_with_eos = list(targets) + [EOS]\n",
    "        yield np.array(inputs_with_eos), np.array(targets_with_eos)\n",
    "\n",
    "tokenized_train_stream = append_eos(tokenized_train_stream)\n",
    "tokenized_eval_stream = append_eos(tokenized_eval_stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## filter long sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mSingle tokenized example input:\u001b[0m [ 2538  2248    30 12114 23184 16889     5     2 20852  6456 20592  5812\n",
      "  3932    96  5178  3851    30  7891  3550 30650  4729   992     1]\n",
      "\u001b[31mSingle tokenized example target:\u001b[0m [ 1872    11  3544    39  7019 17877 30432    23  6845    10 14222    47\n",
      "  4004    18 21674     5 27467  9513   920   188 10630    18  3550 30650\n",
      "  4729   992     1]\n"
     ]
    }
   ],
   "source": [
    "filtered_train_stream = trax.data.FilterByLength(\n",
    "    max_length=512, length_keys=[0, 1])(tokenized_train_stream)\n",
    "filtered_eval_stream = trax.data.FilterByLength(\n",
    "    max_length=512, length_keys=[0, 1])(tokenized_eval_stream)\n",
    "\n",
    "train_input, train_target = next(filtered_train_stream)\n",
    "print(colored(f'Single tokenized example input:', 'red'), train_input)\n",
    "print(colored(f'Single tokenized example target:', 'red'), train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(input_str, vocab_file=None, vocab_dir=None):\n",
    "    EOS = 1\n",
    "    inputs = next(trax.data.tokenize(iter([input_str]), vocab_file=vocab_file, vocab_dir=vocab_dir))\n",
    "    inputs = list(inputs) + [EOS]\n",
    "\n",
    "    batch_inputs = np.reshape(np.array(inputs), [1, -1])\n",
    "    \n",
    "    return batch_inputs\n",
    "\n",
    "def detokenize(integers, vocab_file=None, vocab_dir=None):\n",
    "    integers = list(np.squeeze(integers))\n",
    "\n",
    "    EOS = 1\n",
    "\n",
    "    if EOS in integers:\n",
    "        integers = integers[:integers.index(EOS)]\n",
    "    return trax.data.detokenize(integers, vocab_file=vocab_file, vocab_dir=vocab_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mSingle detokenized example input:\u001b[0m During treatment with olanzapine, adolescents gained significantly more weight compared with adults.\n",
      "\n",
      "\u001b[31mSingle detokenized example target:\u001b[0m WÃ¤hrend der Behandlung mit Olanzapin nahmen die Jugendlichen im Vergleich zu Erwachsenen signifikant mehr Gewicht zu.\n",
      "\n",
      "\u001b[32mtokenize('hello'): \u001b[0m [[17332   140     1]]\n",
      "\u001b[32mdetokenize([17332, 140, 1]): \u001b[0m hello\n"
     ]
    }
   ],
   "source": [
    "print(colored(f'Single detokenized example input:', 'red'), detokenize(train_input, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR))\n",
    "print(colored(f'Single detokenized example target:', 'red'), detokenize(train_target, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR))\n",
    "\n",
    "print(colored(f\"tokenize('hello'): \", 'green'), tokenize('hello', vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR))\n",
    "print(colored(f\"detokenize([17332, 140, 1]): \", 'green'), detokenize([17332, 140, 1], vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bucketing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "boundaries = [8, 16, 32, 64, 128, 256, 512]\n",
    "batch_sizes = [256, 128, 64, 32, 16, 8, 4, 2]\n",
    "\n",
    "train_batch_stream = trax.data.BucketByLength(\n",
    "    boundaries, batch_sizes,\n",
    "    length_keys=[0, 1]\n",
    ")(filtered_train_stream)\n",
    "\n",
    "eval_batch_stream = trax.data.BucketByLength(\n",
    "    boundaries, batch_sizes,\n",
    "    length_keys=[0, 1]\n",
    ")(filtered_eval_stream)\n",
    "\n",
    "train_batch_stream = trax.data.AddLossWeights(id_to_mask=0)(train_batch_stream)\n",
    "eval_batch_stream = trax.data.AddLossWeights(id_to_mask=0)(eval_batch_stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_batch data type:  <class 'numpy.ndarray'>\n",
      "target_batch data type:  <class 'numpy.ndarray'>\n",
      "input_batch shape:  (32, 64)\n",
      "target_batch shape:  (32, 64)\n"
     ]
    }
   ],
   "source": [
    "input_batch, target_batch, mask_batch = next(train_batch_stream)\n",
    "\n",
    "print('input_batch data type: ', type(input_batch))\n",
    "print('target_batch data type: ', type(target_batch))\n",
    "\n",
    "print(\"input_batch shape: \", input_batch.shape)\n",
    "print(\"target_batch shape: \", target_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mTHIS IS THE ENGLISH SENTENCE: \n",
      "\u001b[0m Prior treatment with high dose diuretics may result in volume depletion and a risk of hypotension when initiating therapy with Karvea (see section 4.4).\n",
      " \n",
      "\n",
      "\u001b[31mTHIS IS THE TOKENIZED VERSION OF THE ENGLISH SENTENCE: \n",
      " \u001b[0m [ 5120    66  2248    30   350 20441  4296 22371  8141    14   247   571\n",
      "     6  7618  6530  6858   379     8    13   814     7  4814 23010 12122\n",
      "    22   196 28753  1333 14667    30  4345  3452    13    50   372  2745\n",
      "   219     3   219 33022 30650  4729   992     1     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0] \n",
      "\n",
      "\u001b[31mTHIS IS THE GERMAN TRANSLATION: \n",
      "\u001b[0m Eine Vorbehandlung mit hohen Dosen von Diuretika kann bei Beginn der Therapie mit Karvea zu FlÃ¼ssigkeitsmangel und zum Risiko eines Ã¼bermÃ¤Ãigen Blutdruckabfalls fÃ¼hren (siehe Abschnitt 4.4).\n",
      " \n",
      "\n",
      "\u001b[31mTHIS IS THE TOKENIZED VERSION OF THE GERMAN TRANSLATION: \n",
      "\u001b[0m [  478  8427 12305     5    39  2514 14327    28    21  5151 22371  4581\n",
      "  2694   136   113  2076    11 15761 24780    39  4345  3452    13    18\n",
      " 32165  5207 15226 13726 13484     5    12   130  3527   225 13032    23\n",
      "  9208 29697 17944 13939     5   750    50  4588 10729   219     3   219\n",
      " 33022 30650  4729   992     1     0     0     0     0     0     0     0\n",
      "     0     0     0     0] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pick a random index less than the batch size.\n",
    "index = random.randrange(len(input_batch))\n",
    "\n",
    "# use the index to grab an entry from the input and target batch\n",
    "print(colored('THIS IS THE ENGLISH SENTENCE: \\n', 'red'), detokenize(input_batch[index], vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR), '\\n')\n",
    "print(colored('THIS IS THE TOKENIZED VERSION OF THE ENGLISH SENTENCE: \\n ', 'red'), input_batch[index], '\\n')\n",
    "print(colored('THIS IS THE GERMAN TRANSLATION: \\n', 'red'), detokenize(target_batch[index], vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR), '\\n')\n",
    "print(colored('THIS IS THE TOKENIZED VERSION OF THE GERMAN TRANSLATION: \\n', 'red'), target_batch[index], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NMT with Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## input encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_encoder_fn(input_vocab_size, d_model, n_encoder_layers):\n",
    "    input_encoder = tl.Serial(\n",
    "        # convert tokens to vectors\n",
    "        tl.Embedding(input_vocab_size, d_model),\n",
    "\n",
    "        # feed embedding to lstm\n",
    "        [tl.LSTM(d_model) for _ in range(n_encoder_layers)]\n",
    "    )\n",
    "    return input_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "# test input_encoder_fn\n",
    "w1_unittest.test_input_encoder_fn(input_encoder_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-attention decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_attention_decoder_fn(mode, target_vocab_size, d_model):\n",
    "    pre_attention_decoder = tl.Serial(\n",
    "        tl.ShiftRight(mode=mode),\n",
    "        tl.Embedding(target_vocab_size, d_model),\n",
    "        tl.LSTM(d_model)\n",
    "    )\n",
    "    return pre_attention_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "w1_unittest.test_pre_attention_decoder_fn(pre_attention_decoder_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare attention input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_attention_input(encoder_activations, decoder_activations, inputs):\n",
    "    keys = encoder_activations\n",
    "    values = encoder_activations\n",
    "\n",
    "    queries = decoder_activations\n",
    "\n",
    "    mask = fastnp.where(inputs>0, 1, 0)\n",
    "\n",
    "    mask = fastnp.reshape(mask, (mask.shape[0], 1, 1, mask.shape[1]))\n",
    "    \n",
    "    # broadcast so mask shape is [batch size, attention heads, decoder-len, encoder-len].\n",
    "    # note: for this assignment, attention heads is set to 1.\n",
    "    mask = mask + fastnp.zeros((1, 1, decoder_activations.shape[1], 1))\n",
    "        \n",
    "    \n",
    "    return queries, keys, values, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "w1_unittest.test_prepare_attention_input(prepare_attention_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMTAttn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NMTAttn(input_vocab_size=33300,\n",
    "            target_vocab_size=33300,\n",
    "            d_model=1024,\n",
    "            n_encoder_layers=2,\n",
    "            n_decoder_layers=2,\n",
    "            n_attention_heads=4,\n",
    "            attention_dropout=0.0,\n",
    "            mode='train'):\n",
    "    # Step 0\n",
    "    input_encoder = input_encoder_fn(input_vocab_size, d_model, n_encoder_layers)\n",
    "    pre_attention_decoder = pre_attention_decoder_fn(mode, target_vocab_size, d_model)\n",
    "\n",
    "    # Step 1\n",
    "    model = tl.Serial(\n",
    "        # Step 2\n",
    "        tl.Select([0, 1, 0, 1]),\n",
    "\n",
    "        # Step 3\n",
    "        tl.Parallel(input_encoder, pre_attention_decoder),\n",
    "\n",
    "        # Step 4\n",
    "        tl.Fn('PrepareAttentionInput', prepare_attention_input, n_out=4),\n",
    "\n",
    "        # Step 5\n",
    "        tl.Residual(tl.AttentionQKV(d_model, n_heads=n_attention_heads, dropout=attention_dropout, mode=mode)),\n",
    "\n",
    "        # Step 6\n",
    "        tl.Select([0, 2]),\n",
    "        \n",
    "        # Step 7\n",
    "        [tl.LSTM(d_model) for _ in range(n_decoder_layers)],\n",
    "\n",
    "        # Step 8\n",
    "        tl.Dense(target_vocab_size),\n",
    "\n",
    "        # Step 9\n",
    "        tl.LogSoftmax()\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serial_in2_out2[\n",
      "  Select[0,1,0,1]_in2_out4\n",
      "  Parallel_in2_out2[\n",
      "    Serial[\n",
      "      Embedding_33300_1024\n",
      "      LSTM_1024\n",
      "      LSTM_1024\n",
      "    ]\n",
      "    Serial[\n",
      "      Serial[\n",
      "        ShiftRight(1)\n",
      "      ]\n",
      "      Embedding_33300_1024\n",
      "      LSTM_1024\n",
      "    ]\n",
      "  ]\n",
      "  PrepareAttentionInput_in3_out4\n",
      "  Serial_in4_out2[\n",
      "    Branch_in4_out3[\n",
      "      None\n",
      "      Serial_in4_out2[\n",
      "        _in4_out4\n",
      "        Serial_in4_out2[\n",
      "          Parallel_in3_out3[\n",
      "            Dense_1024\n",
      "            Dense_1024\n",
      "            Dense_1024\n",
      "          ]\n",
      "          PureAttention_in4_out2\n",
      "          Dense_1024\n",
      "        ]\n",
      "        _in2_out2\n",
      "      ]\n",
      "    ]\n",
      "    Add_in2\n",
      "  ]\n",
      "  Select[0,2]_in3_out2\n",
      "  LSTM_1024\n",
      "  LSTM_1024\n",
      "  Dense_33300\n",
      "  LogSoftmax\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "model = NMTAttn()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "w1_unittest.test_NMTAttn(NMTAttn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_task_function(train_batch_stream):\n",
    "    return training.TrainTask(\n",
    "        labeled_data=train_batch_stream,\n",
    "        loss_layer=tl.CrossEntropyLoss(),\n",
    "        optimizer=trax.optimizers.Adam(0.01),\n",
    "        lr_schedule=trax.lr.warmup_and_rsqrt_decay(1000, 0.01),\n",
    "        n_steps_per_checkpoint=10\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_task = train_task_function(train_batch_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "w1_unittest.test_train_task(train_task_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_task = training.EvalTask(\n",
    "    \n",
    "    ## use the eval batch stream as labeled data\n",
    "    labeled_data=eval_batch_stream,\n",
    "    \n",
    "    ## use the cross entropy loss and accuracy as metrics\n",
    "    metrics=[tl.CrossEntropyLoss(), tl.Accuracy()],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.9/site-packages/jax/_src/xla_bridge.py:851: UserWarning: jax.host_count has been renamed to jax.process_count. This alias will eventually be removed; please update your code.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# define the output directory\n",
    "output_dir = './output_dir/'\n",
    "\n",
    "# remove old model if it exists. restarts training.\n",
    "!rm -f ./output_dir/model.pkl.gz  \n",
    "\n",
    "# define the training loop\n",
    "training_loop = training.Loop(NMTAttn(mode='train'),\n",
    "                              train_task,\n",
    "                              eval_tasks=[eval_task],\n",
    "                              output_dir=output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.9/site-packages/trax/layers/base.py:851: FutureWarning: GzipFile was opened for writing, but this will change in future Python releases.  Specify the mode argument for opening it for writing.\n",
      "  with gzip.GzipFile(fileobj=f, compresslevel=compresslevel) as gzipf:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step      1: Total number of trainable weights: 148492820\n",
      "Step      1: Ran 1 train steps in 41.89 secs\n",
      "Step      1: train CrossEntropyLoss |  10.43066597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.9/site-packages/trax/supervised/training.py:1249: FutureWarning: GzipFile was opened for writing, but this will change in future Python releases.  Specify the mode argument for opening it for writing.\n",
      "  with gzip_lib.GzipFile(fileobj=f, compresslevel=2) as gzipf:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step      1: eval  CrossEntropyLoss |  10.43288612\n",
      "Step      1: eval          Accuracy |  0.00000000\n",
      "\n",
      "Step     10: Ran 9 train steps in 131.56 secs\n",
      "Step     10: train CrossEntropyLoss |  10.26390457\n",
      "Step     10: eval  CrossEntropyLoss |  9.96872807\n",
      "Step     10: eval          Accuracy |  0.02429765\n"
     ]
    }
   ],
   "source": [
    "training_loop.run(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NMTAttn(mode='eval')\n",
    "\n",
    "model.init_from_file('./output_dir/model.pkl.gz', weights_only=True)\n",
    "model = tl.Accelerate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_symbol(NMTAttn, input_tokens, cur_output_tokens, temperature):\n",
    "    token_length = len(cur_output_tokens)\n",
    "    padded_length = np.power(2, int(np.ceil(np.log2(token_length+1))))\n",
    "    padded = cur_output_tokens + [0]*(padded_length - token_length)\n",
    "    padded_with_batch = np.expand_dims(padded, axis=0)\n",
    "\n",
    "    output, _ = NMTAttn((input_tokens, padded_with_batch))\n",
    "    \n",
    "    # (Hint: choose correct indices on the output)\n",
    "    log_probs = output[0, token_length, :]\n",
    "\n",
    "    # get the next symbol by getting a logsoftmax sample (*hint: cast to an int)\n",
    "    symbol = int(tl.logsoftmax_sample(log_probs, temperature))\n",
    "    \n",
    "    return symbol, float(log_probs[symbol])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "w1_unittest.test_next_symbol(next_symbol, NMTAttn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling_decode(input_sentence, NMTAttn = None, temperature=0.0, vocab_file=None, vocab_dir=None, next_symbol=next_symbol, tokenize=tokenize, detokenize=detokenize):\n",
    "    \"\"\"Returns the translated sentence.\n",
    "\n",
    "    Args:\n",
    "        input_sentence (str): sentence to translate.\n",
    "        NMTAttn (tl.Serial): An LSTM sequence-to-sequence model with attention.\n",
    "        temperature (float): parameter for sampling ranging from 0.0 to 1.0.\n",
    "            0.0: same as argmax, always pick the most probable token\n",
    "            1.0: sampling from the distribution (can sometimes say random things)\n",
    "        vocab_file (str): filename of the vocabulary\n",
    "        vocab_dir (str): path to the vocabulary file\n",
    "\n",
    "    Returns:\n",
    "        tuple: (list, str, float)\n",
    "            list of int: tokenized version of the translated sentence\n",
    "            float: log probability of the translated sentence\n",
    "            str: the translated sentence\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # encode the input sentence\n",
    "    input_tokens = tokenize(input_sentence, vocab_file, vocab_dir)\n",
    "    \n",
    "    # initialize an empty the list of output tokens\n",
    "    cur_output_tokens = []\n",
    "    \n",
    "    # initialize an integer that represents the current output index\n",
    "    cur_output = 0\n",
    "    \n",
    "    # Set the encoding of the \"end of sentence\" as 1\n",
    "    EOS = 1\n",
    "    \n",
    "    # check that the current output is not the end of sentence token\n",
    "    while cur_output_tokens != EOS:\n",
    "        \n",
    "        # update the current output token by getting the index of the next word (hint: use next_symbol)\n",
    "        cur_output, log_prob = next_symbol(NMTAttn, input_tokens, cur_output_tokens, temperature)\n",
    "        \n",
    "        # append the current output token to the list of output tokens\n",
    "        cur_output_tokens.append(cur_output)        \n",
    "    \n",
    "    # detokenize the output tokens\n",
    "    sentence = detokenize(cur_output_tokens)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return cur_output_tokens, log_prob, sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "join() argument must be str, bytes, or os.PathLike object, not 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msampling_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mI love languages.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNMTAttn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mVOCAB_FILE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mVOCAB_DIR\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[42], line 23\u001b[0m, in \u001b[0;36msampling_decode\u001b[0;34m(input_sentence, NMTAttn, temperature, vocab_file, vocab_dir, next_symbol, tokenize, detokenize)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the translated sentence.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03m        str: the translated sentence\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m### START CODE HERE ###\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# encode the input sentence\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m input_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_sentence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# initialize an empty the list of output tokens\u001b[39;00m\n\u001b[1;32m     26\u001b[0m cur_output_tokens \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m, in \u001b[0;36mtokenize\u001b[0;34m(input_str, vocab_file, vocab_dir)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize\u001b[39m(input_str, vocab_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, vocab_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m      2\u001b[0m     EOS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m----> 3\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minput_str\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab_dir\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(inputs) \u001b[38;5;241m+\u001b[39m [EOS]\n\u001b[1;32m      6\u001b[0m     batch_inputs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(np\u001b[38;5;241m.\u001b[39marray(inputs), [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/trax/data/tf_inputs.py:432\u001b[0m, in \u001b[0;36mtokenize\u001b[0;34m(stream, keys, vocab_type, vocab_file, vocab_dir, n_reserved_ids)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;129m@debug_data_pipeline\u001b[39m\u001b[38;5;241m.\u001b[39mdebug_pipeline\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize\u001b[39m(stream,\n\u001b[1;32m    406\u001b[0m              keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    409\u001b[0m              vocab_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    410\u001b[0m              n_reserved_ids\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    411\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Tokenize examples from the stream.\u001b[39;00m\n\u001b[1;32m    412\u001b[0m \n\u001b[1;32m    413\u001b[0m \u001b[38;5;124;03m  This function assumes that `stream` generates either strings or tuples/dicts\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;124;03m    integers -- the tokenized version of these strings.\u001b[39;00m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 432\u001b[0m   vocab \u001b[38;5;241m=\u001b[39m \u001b[43m_get_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvocab_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    433\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m stream:\n\u001b[1;32m    434\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(example, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/trax/data/tf_inputs.py:591\u001b[0m, in \u001b[0;36m_get_vocab\u001b[0;34m(vocab_type, vocab_file, vocab_dir, extra_ids)\u001b[0m\n\u001b[1;32m    588\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m text_encoder\u001b[38;5;241m.\u001b[39mByteTextEncoder(num_reserved_ids\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    590\u001b[0m vocab_dir \u001b[38;5;241m=\u001b[39m vocab_dir \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgs://trax-ml/vocabs/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 591\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvocab_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m vocab_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubword\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    594\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m text_encoder\u001b[38;5;241m.\u001b[39mSubwordTextEncoder(path)\n",
      "File \u001b[0;32m/usr/lib/python3.9/posixpath.py:90\u001b[0m, in \u001b[0;36mjoin\u001b[0;34m(a, *p)\u001b[0m\n\u001b[1;32m     88\u001b[0m             path \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m sep \u001b[38;5;241m+\u001b[39m b\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mAttributeError\u001b[39;00m, \u001b[38;5;167;01mBytesWarning\u001b[39;00m):\n\u001b[0;32m---> 90\u001b[0m     \u001b[43mgenericpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_arg_types\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mjoin\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m path\n",
      "File \u001b[0;32m/usr/lib/python3.9/genericpath.py:152\u001b[0m, in \u001b[0;36m_check_arg_types\u001b[0;34m(funcname, *args)\u001b[0m\n\u001b[1;32m    150\u001b[0m         hasbytes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 152\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfuncname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() argument must be str, bytes, or \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    153\u001b[0m                         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mos.PathLike object, not \u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hasstr \u001b[38;5;129;01mand\u001b[39;00m hasbytes:\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt mix strings and bytes in path components\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: join() argument must be str, bytes, or os.PathLike object, not 'NoneType'"
     ]
    }
   ],
   "source": [
    "sampling_decode(\"I love languages.\", NMTAttn=model, temperature=0.0, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
