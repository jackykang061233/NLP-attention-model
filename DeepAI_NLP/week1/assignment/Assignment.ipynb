{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from termcolor import colored\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import trax\n",
    "from trax import layers as tl\n",
    "from trax.fastmath import numpy as fastnp\n",
    "from trax.supervised import training\n",
    "\n",
    "import w1_unittest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kangchieh/opt/anaconda3/lib/python3.8/site-packages/jax/_src/xla_bridge.py:833: UserWarning: jax.host_count has been renamed to jax.process_count. This alias will eventually be removed; please update your code.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_stream_fn = trax.data.TFDS('opus/medical',\n",
    "                                 data_dir='./data/',\n",
    "                                 keys=('en', 'de'),\n",
    "                                 eval_holdout_size=0.01,\n",
    "                                 train=True\n",
    "                                 )\n",
    "eval_stream_fn = trax.data.TFDS('opus/medical',\n",
    "                                 data_dir='./data/',\n",
    "                                 keys=('en', 'de'),\n",
    "                                 eval_holdout_size=0.01,\n",
    "                                 train=False\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mtrain data (en, de) tuple:\u001b[0m (b'Tel: +421 2 57 103 777\\n', b'Tel: +421 2 57 103 777\\n')\n",
      "\n",
      "\u001b[31meval data (en, de) tuple:\u001b[0m (b'Subcutaneous use and intravenous use.\\n', b'Subkutane Anwendung und intraven\\xc3\\xb6se Anwendung.\\n')\n"
     ]
    }
   ],
   "source": [
    "train_stream = train_stream_fn()\n",
    "print(colored('train data (en, de) tuple:', 'red'), next(train_stream))\n",
    "print()\n",
    "\n",
    "eval_stream = eval_stream_fn()\n",
    "print(colored('eval data (en, de) tuple:', 'red'), next(eval_stream))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_FILE = 'ende_32k.subword'\n",
    "VOCAB_DIR = 'data/'\n",
    "\n",
    "tokenized_train_stream = trax.data.Tokenize(vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)(train_stream)\n",
    "tokenized_eval_stream = trax.data.Tokenize(vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)(eval_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append EOS at the end of each sentence\n",
    "EOS = 1\n",
    "\n",
    "def append_eos(stream):\n",
    "    for (inputs, targets) in stream:\n",
    "        inputs_with_eos = list(inputs) + [EOS]\n",
    "        targets_with_eos = list(targets) + [EOS]\n",
    "        yield np.array(inputs_with_eos), np.array(targets_with_eos)\n",
    "\n",
    "tokenized_train_stream = append_eos(tokenized_train_stream)\n",
    "tokenized_eval_stream = append_eos(tokenized_eval_stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## filter long sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mSingle tokenized example input:\u001b[0m [ 5148 10018   487  1646    64  5024 24618  5748    23  3445 23306     5\n",
      " 30650  4729   992     1]\n",
      "\u001b[31mSingle tokenized example target:\u001b[0m [10609 24618  5748    23  3445 23306     5  3550 30650  4729   992     1]\n"
     ]
    }
   ],
   "source": [
    "filtered_train_stream = trax.data.FilterByLength(\n",
    "    max_length=512, length_keys=[0, 1])(tokenized_train_stream)\n",
    "filtered_eval_stream = trax.data.FilterByLength(\n",
    "    max_length=512, length_keys=[0, 1])(tokenized_eval_stream)\n",
    "\n",
    "train_input, train_target = next(filtered_train_stream)\n",
    "print(colored(f'Single tokenized example input:', 'red'), train_input)\n",
    "print(colored(f'Single tokenized example target:', 'red'), train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(input_str, vocab_file=None, vocab_dir=None):\n",
    "    EOS = 1\n",
    "    inputs = next(trax.data.tokenize(iter([input_str]), vocab_file=vocab_file, vocab_dir=vocab_dir))\n",
    "    inputs = list(inputs) + [EOS]\n",
    "\n",
    "    batch_inputs = np.reshape(np.array(inputs), [1, -1])\n",
    "    \n",
    "    return batch_inputs\n",
    "\n",
    "def detokenize(integers, vocab_file=None, vocab_dir=None):\n",
    "    integers = list(np.squeeze(integers))\n",
    "\n",
    "    EOS = 1\n",
    "\n",
    "    if EOS in integers:\n",
    "        integers = integers[:integers.index(EOS)]\n",
    "    return trax.data.detokenize(integers, vocab_file=vocab_file, vocab_dir=vocab_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mSingle detokenized example input:\u001b[0m Each tablet contains: irbesartan 150 mg\n",
      "\n",
      "\u001b[31mSingle detokenized example target:\u001b[0m Irbesartan 150 mg.\n",
      "\n",
      "\u001b[32mtokenize('hello'): \u001b[0m [[17332   140     1]]\n",
      "\u001b[32mdetokenize([17332, 140, 1]): \u001b[0m hello\n"
     ]
    }
   ],
   "source": [
    "print(colored(f'Single detokenized example input:', 'red'), detokenize(train_input, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR))\n",
    "print(colored(f'Single detokenized example target:', 'red'), detokenize(train_target, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR))\n",
    "\n",
    "print(colored(f\"tokenize('hello'): \", 'green'), tokenize('hello', vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR))\n",
    "print(colored(f\"detokenize([17332, 140, 1]): \", 'green'), detokenize([17332, 140, 1], vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bucketing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "boundaries = [8, 16, 32, 64, 128, 256, 512]\n",
    "batch_sizes = [256, 128, 64, 32, 16, 8, 4, 2]\n",
    "\n",
    "train_batch_stream = trax.data.BucketByLength(\n",
    "    boundaries, batch_sizes,\n",
    "    length_keys=[0, 1]\n",
    ")(filtered_train_stream)\n",
    "\n",
    "eval_batch_stream = trax.data.BucketByLength(\n",
    "    boundaries, batch_sizes,\n",
    "    length_keys=[0, 1]\n",
    ")(filtered_eval_stream)\n",
    "\n",
    "train_batch_stream = trax.data.AddLossWeights(id_to_mask=0)(train_batch_stream)\n",
    "eval_batch_stream = trax.data.AddLossWeights(id_to_mask=0)(eval_batch_stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_batch data type:  <class 'numpy.ndarray'>\n",
      "target_batch data type:  <class 'numpy.ndarray'>\n",
      "input_batch shape:  (32, 64)\n",
      "target_batch shape:  (32, 64)\n"
     ]
    }
   ],
   "source": [
    "input_batch, target_batch, mask_batch = next(train_batch_stream)\n",
    "\n",
    "print('input_batch data type: ', type(input_batch))\n",
    "print('target_batch data type: ', type(target_batch))\n",
    "\n",
    "print(\"input_batch shape: \", input_batch.shape)\n",
    "print(\"target_batch shape: \", target_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mTHIS IS THE ENGLISH SENTENCE: \n",
      "\u001b[0m DATE OF FIRST AUTHORISATION/RENEWAL OF THE AUTHORISATION\n",
      " \n",
      "\n",
      "\u001b[31mTHIS IS THE TOKENIZED VERSION OF THE ENGLISH SENTENCE: \n",
      " \u001b[0m [14344   493 11544  9868 18207   693  9227  7933 10547  3337  4205 10617\n",
      "   123  3234  4017 13986  1047 11544  4327  9227  7933 10547  3337  4205\n",
      " 10617 30650  4729   992     1     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0] \n",
      "\n",
      "\u001b[31mTHIS IS THE GERMAN TRANSLATION: \n",
      "\u001b[0m DATUM DER ERTEILUNG DER ERSTZULASSUNG / VERLÃ„NGERUNG DER ZULASSUNG\n",
      " \n",
      "\n",
      "\u001b[31mTHIS IS THE TOKENIZED VERSION OF THE GERMAN TRANSLATION: \n",
      "\u001b[0m [14344 11459     5  9192  6304  6999  8001 18044  9192  6304 14075  1834\n",
      " 25057  8066  8462 11553  1962 10468  2474  3449  9797  6304 18044  9192\n",
      "  1834 25057  8066  8462 11553 30650  4729   992     1     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pick a random index less than the batch size.\n",
    "index = random.randrange(len(input_batch))\n",
    "\n",
    "# use the index to grab an entry from the input and target batch\n",
    "print(colored('THIS IS THE ENGLISH SENTENCE: \\n', 'red'), detokenize(input_batch[index], vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR), '\\n')\n",
    "print(colored('THIS IS THE TOKENIZED VERSION OF THE ENGLISH SENTENCE: \\n ', 'red'), input_batch[index], '\\n')\n",
    "print(colored('THIS IS THE GERMAN TRANSLATION: \\n', 'red'), detokenize(target_batch[index], vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR), '\\n')\n",
    "print(colored('THIS IS THE TOKENIZED VERSION OF THE GERMAN TRANSLATION: \\n', 'red'), target_batch[index], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NMT with Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## input encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_encoder_fn(input_vocab_size, d_model, n_encoder_layers):\n",
    "    input_encoder = tl.Serial(\n",
    "        # convert tokens to vectors\n",
    "        tl.Embedding(input_vocab_size, d_model),\n",
    "\n",
    "        # feed embedding to lstm\n",
    "        [tl.LSTM(d_model) for _ in range(n_encoder_layers)]\n",
    "    )\n",
    "    return input_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "# test input_encoder_fn\n",
    "w1_unittest.test_input_encoder_fn(input_encoder_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-attention decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_attention_decoder_fn(mode, target_vocab_size, d_model):\n",
    "    pre_attention_decoder = tl.Serial(\n",
    "        tl.ShiftRight(mode=mode),\n",
    "        tl.Embedding(target_vocab_size, d_model),\n",
    "        tl.LSTM(d_model)\n",
    "    )\n",
    "    return pre_attention_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "w1_unittest.test_pre_attention_decoder_fn(pre_attention_decoder_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare attention input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_attention_input(encoder_activations, decoder_activations, inputs):\n",
    "    keys = encoder_activations\n",
    "    values = encoder_activations\n",
    "\n",
    "    queries = decoder_activations\n",
    "\n",
    "    mask = fastnp.where(inputs>0, 1, 0)\n",
    "\n",
    "    mask = fastnp.reshape(mask, (mask.shape[0], 1, 1, mask.shape[1]))\n",
    "    \n",
    "    # broadcast so mask shape is [batch size, attention heads, decoder-len, encoder-len].\n",
    "    # note: for this assignment, attention heads is set to 1.\n",
    "    mask = mask + fastnp.zeros((1, 1, decoder_activations.shape[1], 1))\n",
    "        \n",
    "    \n",
    "    return queries, keys, values, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "w1_unittest.test_prepare_attention_input(prepare_attention_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMTAttn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NMTAttn(input_vocab_size=33300,\n",
    "            target_vocab_size=33300,\n",
    "            d_model=1024,\n",
    "            n_encoder_layers=2,\n",
    "            n_decoder_layers=2,\n",
    "            n_attention_heads=4,\n",
    "            attention_dropout=0.0,\n",
    "            mode='train'):\n",
    "    # Step 0\n",
    "    input_encoder = input_encoder_fn(input_vocab_size, d_model, n_encoder_layers)\n",
    "    pre_attention_decoder = pre_attention_decoder_fn(mode, target_vocab_size, d_model)\n",
    "\n",
    "    # Step 1\n",
    "    model = tl.Serial(\n",
    "        # Step 2\n",
    "        tl.Select([0, 1, 0, 1]),\n",
    "\n",
    "        # Step 3\n",
    "        tl.Parallel(input_encoder, pre_attention_decoder),\n",
    "\n",
    "        # Step 4\n",
    "        tl.Fn('PrepareAttentionInput', prepare_attention_input, n_out=4),\n",
    "\n",
    "        # Step 5\n",
    "        tl.Residual(tl.AttentionQKV(d_model, n_heads=n_attention_heads, dropout=attention_dropout, mode=mode)),\n",
    "\n",
    "        # Step 6\n",
    "        tl.Select([0, 2]),\n",
    "        \n",
    "        # Step 7\n",
    "        [tl.LSTM(d_model) for _ in range(n_decoder_layers)],\n",
    "\n",
    "        # Step 8\n",
    "        tl.Dense(target_vocab_size),\n",
    "\n",
    "        # Step 9\n",
    "        tl.LogSoftmax()\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serial_in2_out2[\n",
      "  Select[0,1,0,1]_in2_out4\n",
      "  Parallel_in2_out2[\n",
      "    Serial[\n",
      "      Embedding_33300_1024\n",
      "      LSTM_1024\n",
      "      LSTM_1024\n",
      "    ]\n",
      "    Serial[\n",
      "      Serial[\n",
      "        ShiftRight(1)\n",
      "      ]\n",
      "      Embedding_33300_1024\n",
      "      LSTM_1024\n",
      "    ]\n",
      "  ]\n",
      "  PrepareAttentionInput_in3_out4\n",
      "  Serial_in4_out2[\n",
      "    Branch_in4_out3[\n",
      "      None\n",
      "      Serial_in4_out2[\n",
      "        _in4_out4\n",
      "        Serial_in4_out2[\n",
      "          Parallel_in3_out3[\n",
      "            Dense_1024\n",
      "            Dense_1024\n",
      "            Dense_1024\n",
      "          ]\n",
      "          PureAttention_in4_out2\n",
      "          Dense_1024\n",
      "        ]\n",
      "        _in2_out2\n",
      "      ]\n",
      "    ]\n",
      "    Add_in2\n",
      "  ]\n",
      "  Select[0,2]_in3_out2\n",
      "  LSTM_1024\n",
      "  LSTM_1024\n",
      "  Dense_33300\n",
      "  LogSoftmax\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "model = NMTAttn()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "w1_unittest.test_NMTAttn(NMTAttn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_task_function(train_batch_stream):\n",
    "    return training.TrainTask(\n",
    "        labeled_data=train_batch_stream,\n",
    "        loss_layer=tl.CrossEntropyLoss(),\n",
    "        optimizer=trax.optimizers.Adam(0.01),\n",
    "        lr_schedule=trax.lr.warmup_and_rsqrt_decay(1000, 0.01),\n",
    "        n_steps_per_checkpoint=10\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_task = train_task_function(train_batch_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "w1_unittest.test_train_task(train_task_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_task = training.EvalTask(\n",
    "    labeled_data=eval_batch_stream,\n",
    "    metrics=[tl.CrossEntropyLoss(), tl.Accuracy()],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kangchieh/opt/anaconda3/lib/python3.8/site-packages/jax/_src/xla_bridge.py:833: UserWarning: jax.host_count has been renamed to jax.process_count. This alias will eventually be removed; please update your code.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "output_dir = './output_dir/'\n",
    "!rm -f ~/output_dir/model.pkg.gz\n",
    "\n",
    "training_loop = training.Loop(NMTAttn(mode='train'),\n",
    "                              train_task,\n",
    "                              eval_tasks=[eval_task],\n",
    "                              output_dir=output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step      1: Total number of trainable weights: 148492820\n",
      "Step      1: Ran 1 train steps in 87.48 secs\n",
      "Step      1: train CrossEntropyLoss |  10.40531635\n",
      "Step      1: eval  CrossEntropyLoss |  10.39260292\n",
      "Step      1: eval          Accuracy |  0.00000000\n",
      "\n",
      "Step     10: Ran 9 train steps in 175.00 secs\n",
      "Step     10: train CrossEntropyLoss |  10.24884701\n",
      "Step     10: eval  CrossEntropyLoss |  9.95088005\n",
      "Step     10: eval          Accuracy |  0.02429765\n"
     ]
    }
   ],
   "source": [
    "training_loop.run(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NMTAttn(mode='eval')\n",
    "\n",
    "model.init_from_file('./output_dir/model.pkl.gz', weights_only=True)\n",
    "model = tl.Accelerate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
